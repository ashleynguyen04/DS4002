{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashleynguyen04/DS4002/blob/main/project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating dataset\n",
        "#Download \"Large Movie Review Dataset v1.0\" from https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "#Download \"title.basics.tsv.gz\" from https://datasets.imdbws.com/\n",
        "#Upload aclImd zip file and extracted title.basic.tsv to colab\n",
        "\n",
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "# CONFIGURE PATHS\n",
        "ACL_ARCHIVE_TGZ = Path(\"/aclImdb_v1.tar.gz\")   #copy and paste the path\n",
        "ACL_ARCHIVE_ZIP = None\n",
        "TITLE_BASICS_PATH = Path(\"/title.basics.tsv\") # #copy and paste the path\n",
        "ACL_ROOT_DIR = Path(\"/content/aclImdb\") # will exist after aclIMDb is unzipped\n",
        "OUTPUT_PARQUET = Path(\"/content/imdb_sentiment_with_genres.parquet\")\n",
        "OUTPUT_CSV = Path(\"/content/imdb_sentiment_with_genres.csv\")\n",
        "WRITE_CSV_TOO = False\n",
        "\n",
        "TT_RE = re.compile(r\"(tt\\d+)\")\n",
        "STAR_RE = re.compile(r\"_(\\d+)\\.txt$\", re.IGNORECASE)\n",
        "\n",
        "def extract_archive_if_needed(target_dir: Path):\n",
        "    if target_dir.exists() and (target_dir / \"train\").exists() and (target_dir / \"test\").exists():\n",
        "        print(f\"Found existing {target_dir}, skipping extraction.\")\n",
        "        return\n",
        "    if ACL_ARCHIVE_TGZ and Path(ACL_ARCHIVE_TGZ).exists():\n",
        "        print(f\"Extracting {ACL_ARCHIVE_TGZ} -> /content/ ...\")\n",
        "        with tarfile.open(ACL_ARCHIVE_TGZ, \"r:gz\") as tf:\n",
        "            tf.extractall(\"/content/\")\n",
        "        print(\"Extraction (tar.gz) complete.\")\n",
        "    elif ACL_ARCHIVE_ZIP and Path(ACL_ARCHIVE_ZIP).exists():\n",
        "        print(f\"Extracting {ACL_ARCHIVE_ZIP} -> /content/ ...\")\n",
        "        with zipfile.ZipFile(ACL_ARCHIVE_ZIP, \"r\") as zf:\n",
        "            zf.extractall(\"/content/\")\n",
        "        print(\"Extraction (zip) complete.\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Upload aclImdb_v1.tar.gz (or aclImdb.zip) to /content.\")\n",
        "\n",
        "def find_acl_root(base: Path) -> Path:\n",
        "    def is_acl_root(p: Path) -> bool:\n",
        "        return (p / \"train\" / \"pos\").exists() and (p / \"train\" / \"neg\").exists() \\\n",
        "           and (p / \"test\" / \"pos\").exists() and (p / \"test\" / \"neg\").exists()\n",
        "    if is_acl_root(base):\n",
        "        return base\n",
        "    for root, dirs, files in os.walk(base):\n",
        "        p = Path(root)\n",
        "        if is_acl_root(p):\n",
        "            return p\n",
        "    raise RuntimeError(f\"Could not find aclImdb root with train/test pos/neg under: {base}\")\n",
        "\n",
        "def read_split_sentiment(split_dir: Path, sentiment: str) -> pd.DataFrame:\n",
        "    reviews_dir = split_dir / sentiment\n",
        "    urls_file = split_dir / f\"urls_{sentiment}.txt\"\n",
        "\n",
        "    review_files = sorted([p for p in reviews_dir.glob(\"*.txt\") if p.is_file()])\n",
        "    if not review_files:\n",
        "        print(f\"WARNING: No review files in {reviews_dir}\")\n",
        "        return pd.DataFrame(columns=[\"review_path\", \"review\", \"rating\", \"tt_id\", \"review_stars\"])\n",
        "\n",
        "    if not urls_file.exists():\n",
        "        raise FileNotFoundError(f\"Missing URLs file: {urls_file}\")\n",
        "\n",
        "    with urls_file.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        urls = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    n = min(len(urls), len(review_files))\n",
        "    if len(urls) != len(review_files):\n",
        "        print(f\"NOTE: {split_dir.name}-{sentiment} count mismatch. URLs={len(urls)} reviews={len(review_files)}. Using {n}.\")\n",
        "    urls = urls[:n]\n",
        "    review_files = review_files[:n]\n",
        "\n",
        "    tt_ids, rows = [], []\n",
        "    for u in urls:\n",
        "        m = TT_RE.search(u)\n",
        "        tt_ids.append(m.group(1) if m else None)\n",
        "\n",
        "    for fpath, tt in zip(review_files, tt_ids):\n",
        "        with fpath.open(\"r\", encoding=\"utf-8\") as rf:\n",
        "            text = rf.read().strip()\n",
        "        mstar = STAR_RE.search(fpath.name)\n",
        "        star = int(mstar.group(1)) if mstar else None\n",
        "        rows.append({\n",
        "            \"review_path\": str(fpath),\n",
        "            \"review\": text,\n",
        "            \"rating\": \"pos\" if sentiment == \"pos\" else \"neg\",\n",
        "            \"tt_id\": tt,\n",
        "            \"review_stars\": star\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def load_acl_imdb_all(acl_root: Path) -> pd.DataFrame:\n",
        "    dfs = []\n",
        "    for split in [\"train\", \"test\"]:\n",
        "        sd = acl_root / split\n",
        "        if not sd.exists():\n",
        "            print(f\"WARNING: Missing split dir {sd}\")\n",
        "            continue\n",
        "        for sentiment in [\"pos\", \"neg\"]:\n",
        "            dfs.append(read_split_sentiment(sd, sentiment))\n",
        "    if not dfs:\n",
        "        return pd.DataFrame(columns=[\"review_path\", \"review\", \"rating\", \"tt_id\", \"review_stars\"])\n",
        "    return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "def load_title_basics(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path, sep=\"\\t\", dtype=str, na_values=\"\\\\N\")\n",
        "    df = df[[\"tconst\", \"primaryTitle\", \"genres\"]]\n",
        "    df[\"genres\"] = df[\"genres\"].fillna(\"\")\n",
        "    return df\n",
        "\n",
        "#PIPELINE TO MERGE DATASET\n",
        "try:\n",
        "    extract_archive_if_needed(ACL_ROOT_DIR)\n",
        "    acl_root = find_acl_root(ACL_ROOT_DIR)\n",
        "    print(f\"Using ACL root: {acl_root}\")\n",
        "\n",
        "    print(\"Loading ACL IMDB reviews…\")\n",
        "    reviews_df = load_acl_imdb_all(acl_root)\n",
        "    if reviews_df.empty:\n",
        "        raise RuntimeError(\"No reviews loaded.\")\n",
        "\n",
        "    print(\"Loading IMDb title.basics.tsv…\")\n",
        "    titles_df = load_title_basics(TITLE_BASICS_PATH)\n",
        "\n",
        "    print(\"Merging on IMDb IDs…\")\n",
        "    reviews_df = reviews_df.dropna(subset=[\"tt_id\"])\n",
        "    need_ids = reviews_df[\"tt_id\"].unique()\n",
        "    titles_small = titles_df[titles_df[\"tconst\"].isin(need_ids)].copy()\n",
        "\n",
        "    merged = reviews_df.merge(titles_small, left_on=\"tt_id\", right_on=\"tconst\", how=\"left\")\n",
        "\n",
        "    merged[\"movie_name\"] = merged[\"primaryTitle\"].fillna(\"Unknown Title\")\n",
        "    merged[\"genres\"] = merged[\"genres\"].fillna(\"\").astype(str)\n",
        "    merged[\"genres_list\"] = merged[\"genres\"].apply(lambda s: [] if s.strip() == \"\" else s.split(\",\"))\n",
        "    long_df = merged.explode(\"genres_list\", ignore_index=True)\n",
        "    long_df = long_df.rename(columns={\"genres_list\": \"genre\"})\n",
        "    long_df.loc[long_df[\"genre\"] == \"\", \"genre\"] = None\n",
        "\n",
        "    final = long_df.rename(columns={\"tt_id\": \"imdb_identifier\"})[\n",
        "        [\"imdb_identifier\", \"movie_name\", \"genres\", \"genre\", \"review\", \"rating\", \"review_stars\"]\n",
        "    ].copy()\n",
        "\n",
        "    #TYPE CASTING: strings for text columns, Int for review_stars\n",
        "    str_cols = [\"imdb_identifier\", \"movie_name\", \"genres\", \"genre\", \"review\", \"rating\"]\n",
        "    final[str_cols] = final[str_cols].astype(pd.StringDtype())\n",
        "    final[\"review_stars\"] = final[\"review_stars\"].astype(\"Int64\")  # nullable integer\n",
        "\n",
        "    print(final.dtypes)\n",
        "\n",
        "    final.to_parquet(OUTPUT_PARQUET, index=False)\n",
        "    print(f\"Saved Parquet: {OUTPUT_PARQUET}\")\n",
        "\n",
        "    if WRITE_CSV_TOO:\n",
        "        final.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "        print(f\"Saved CSV: {OUTPUT_CSV}\")\n",
        "\n",
        "    # quick peek\n",
        "    print(\"\\nPreview:\")\n",
        "    print(final.head(3))\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\nERROR:\", e)\n",
        "    raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "x-bopyzFRcZO",
        "outputId": "55527581-6aec-41d9-c89a-a626edafd769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ERROR: Upload aclImdb_v1.tar.gz (or aclImdb.zip) to /content.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Upload aclImdb_v1.tar.gz (or aclImdb.zip) to /content.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3896420330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m#PIPELINE TO MERGE DATASET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mextract_archive_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACL_ROOT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0macl_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_acl_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACL_ROOT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Using ACL root: {acl_root}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3896420330.py\u001b[0m in \u001b[0;36mextract_archive_if_needed\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extraction (zip) complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Upload aclImdb_v1.tar.gz (or aclImdb.zip) to /content.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_acl_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Upload aclImdb_v1.tar.gz (or aclImdb.zip) to /content."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your Parquet dataset\n",
        "import pandas as pd\n",
        "df = pd.read_parquet(\"/content/imdb_sentiment_with_genres.parquet\")"
      ],
      "metadata": {
        "id": "Fom4I-4SscbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preview the dataset\n",
        "print(df.shape)\n",
        "print(df.columns)\n",
        "print(df.dtypes)\n",
        "\n",
        "df.head(20)"
      ],
      "metadata": {
        "id": "fdc7pruJ_tUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at what years the movies are from\n",
        "# Reload the title.basics.tsv to get year info\n",
        "titles = pd.read_csv(\"/title.basics.tsv\", sep=\"\\t\", dtype=str, na_values=\"\\\\N\")\n",
        "\n",
        "# Merge startYear onto your dataset\n",
        "df = df.merge(\n",
        "    titles[[\"tconst\", \"startYear\"]],\n",
        "    left_on=\"imdb_identifier\",\n",
        "    right_on=\"tconst\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Clean up\n",
        "df = df.drop(columns=[\"tconst\"])\n",
        "df[\"startYear\"] = df[\"startYear\"].fillna(\"Unknown\")\n",
        "\n",
        "# Check distribution\n",
        "print(df[\"startYear\"].value_counts().head(10))\n"
      ],
      "metadata": {
        "id": "lNuA_FibLVRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EDA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "######## REVIEW STARS BY GENRE #########\n",
        "# Drop rows with missing review_stars\n",
        "genre_ratings = df.dropna(subset=[\"review_stars\"])\n",
        "\n",
        "# Group by genre → average review_stars\n",
        "genre_avg = (genre_ratings\n",
        "             .groupby(\"genre\")[\"review_stars\"]\n",
        "             .mean()\n",
        "             .sort_values(ascending=False))\n",
        "\n",
        "# Plot top 50 genres by average stars\n",
        "plt.figure(figsize=(10,6))\n",
        "genre_avg.head(50).plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
        "plt.title(\"Top 50 Genres by Average Review Stars\")\n",
        "plt.ylabel(\"Average Stars (1–10)\")\n",
        "plt.xlabel(\"Genre\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "######## # OF POS AND NEG REVIEW THROUGHOUT THE YEARS #########\n",
        "# Reload IMDb basics WITH startYear this time\n",
        "titles = pd.read_csv(\n",
        "    \"/title.basics.tsv\",\n",
        "    sep=\"\\t\",\n",
        "    dtype=str,\n",
        "    na_values=\"\\\\N\"\n",
        ")\n",
        "\n",
        "# Merge startYear into df\n",
        "df = df.merge(\n",
        "    titles[[\"tconst\", \"startYear\"]],\n",
        "    left_on=\"imdb_identifier\",\n",
        "    right_on=\"tconst\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Clean up\n",
        "df = df.drop(columns=[\"tconst\"])\n",
        "df[\"startYear\"] = pd.to_numeric(df[\"startYear\"], errors=\"coerce\")\n",
        "\n",
        "print(df[[\"movie_name\", \"startYear\", \"rating\"]].head())\n",
        "\n",
        "yearly_counts = (\n",
        "    df.dropna(subset=[\"startYear\"])\n",
        "      .groupby([\"startYear\", \"rating\"])\n",
        "      .size()\n",
        "      .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "print(yearly_counts.head())\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "for sentiment in [\"pos\", \"neg\"]:\n",
        "    subset = yearly_counts[yearly_counts[\"rating\"] == sentiment]\n",
        "    plt.plot(subset[\"startYear\"], subset[\"count\"], label=sentiment)\n",
        "\n",
        "plt.title(\"Number of Positive vs Negative Reviews by Movie Release Year\")\n",
        "plt.xlabel(\"Release Year\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "plt.legend(title=\"Rating\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "naOwgaMssHgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Naive Bayes Model With Data\n",
        "This will involve selecting the relevant columns and converting the 'rating' column to numerical labels. The code selects the 'review' and 'rating' columns and convert the 'rating' column to numerical labels for model training."
      ],
      "metadata": {
        "id": "kJNukxhkxuIP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38a42b02"
      },
      "source": [
        "# Select relevant columns\n",
        "df_model = df[['review', 'rating']].copy()\n",
        "\n",
        "# Convert 'rating' column to numerical labels\n",
        "df_model['sentiment_label'] = df_model['rating'].apply(lambda x: 1 if x == 'pos' else 0)\n",
        "\n",
        "# Display the first few rows with the new column\n",
        "display(df_model.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data\n",
        "\n",
        "Split the data into training and testing sets. Split the data into training and testing sets using train_test_split.\n"
      ],
      "metadata": {
        "id": "gX3XLmEJx8Y-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "114361c2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_model['review']\n",
        "y = df_model['sentiment_label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize text\n",
        "\n",
        "Convert the text reviews into numerical features using a technique like TF-IDF. Import TfidfVectorizer, instantiate it, fit on training data, transform both training and testing data, and print the shapes of the resulting matrices.\n"
      ],
      "metadata": {
        "id": "Dv-RAhphyJo3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "766d8ee5"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"Shape of X_train_tfidf: {X_train_tfidf.shape}\")\n",
        "print(f\"Shape of X_test_tfidf: {X_test_tfidf.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train naive bayes model\n",
        "\n",
        "Train a Multinomial Naive Bayes model on the training data. Import the necessary class, instantiate the model, and train it on the TF-IDF transformed training data and labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "G2wGa0GJyROn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b17e0dae"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Instantiate a MultinomialNB model\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model\n",
        "Evaluate the trained model's performance on the testing data using appropriate metrics like accuracy, precision, and recall. Make predictions on the test data and evaluate the model's performance using accuracy, precision, and recall.\n"
      ],
      "metadata": {
        "id": "ZcBNbhXLyhDW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18b0955e"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analysis and Key Findings\n",
        "\n",
        "*   The original data was prepared by selecting the 'review' and 'rating' columns and converting the 'rating' column into numerical labels (1 for 'pos', 0 for 'neg') in a new column named 'sentiment\\_label'.\n",
        "*   The data was split into training (80%) and testing (20%) sets, resulting in 91,309 training samples and 22,828 testing samples for both features and labels.\n",
        "*   Text reviews were converted into numerical features using TF-IDF vectorization, resulting in a feature space of 99,326 unique terms learned from the training data.\n",
        "*   A Multinomial Naive Bayes model was trained on the TF-IDF transformed training data.\n",
        "*   The trained Naive Bayes model achieved an accuracy of approximately 89.43\\%, a precision of approximately 90.53\\%, and a recall of approximately 88.39\\% on the test data.\n",
        "\n",
        "*   The Naive Bayes model demonstrates strong performance in classifying movie reviews as \"good\" or \"bad\" based on the calculated metrics.\n"
      ],
      "metadata": {
        "id": "_Eo-YP4LzPHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge data\n",
        "\n",
        "Merge the original DataFrame with the sentiment predictions from the Naive Bayes model. Create a DataFrame from predictions, reset the index of X_test, concatenate them, and merge with the original DataFrame.\n"
      ],
      "metadata": {
        "id": "SwE6DhnpzY8r"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65a1d02f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a DataFrame from y_pred\n",
        "df_pred = pd.DataFrame(y_pred, columns=['predicted_sentiment'])\n",
        "\n",
        "# Reset the index of X_test to ensure alignment\n",
        "X_test_reset = X_test.reset_index(drop=True)\n",
        "\n",
        "# Concatenate X_test_reset and df_pred\n",
        "df_test_predictions = pd.concat([X_test_reset, df_pred], axis=1)\n",
        "df_test_predictions.columns = ['review', 'predicted_sentiment']\n",
        "\n",
        "# Merge with the original DataFrame\n",
        "df_merged = df.merge(df_test_predictions, on='review', how='left')\n",
        "\n",
        "# Display the head of the merged DataFrame\n",
        "display(df_merged.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze genre sentiments\n",
        "\n",
        "Group the merged DataFrame by genre and count the number of good and bad reviews for each genre. Filter the merged dataframe, group by genre, count sentiment, unstack, and fill missing values as instructed.\n",
        "\n"
      ],
      "metadata": {
        "id": "xhkQDdJIzpg4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "716b1ad1"
      },
      "source": [
        "# Filter out rows where predicted_sentiment is NaN\n",
        "df_filtered = df_merged.dropna(subset=['predicted_sentiment']).copy()\n",
        "\n",
        "# Convert predicted_sentiment to integer for grouping\n",
        "df_filtered['predicted_sentiment'] = df_filtered['predicted_sentiment'].astype(int)\n",
        "\n",
        "# Group by genre and count the number of good (1) and bad (0) reviews\n",
        "genre_sentiment_counts = df_filtered.groupby('genre')['predicted_sentiment'].value_counts()\n",
        "\n",
        "# Unstack the results to get a table with genres as rows and sentiments as columns\n",
        "genre_sentiment_table = genre_sentiment_counts.unstack()\n",
        "\n",
        "# Fill any missing values (genres with no good or bad reviews) with 0\n",
        "genre_sentiment_table = genre_sentiment_table.fillna(0)\n",
        "\n",
        "# Display the resulting table\n",
        "display(genre_sentiment_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify top genres\n",
        "\n",
        "Identify the genres with the highest counts of good and bad reviews. Identify the genre with the highest count for both good and bad reviews from the genre_sentiment_table."
      ],
      "metadata": {
        "id": "OF6z6ZCqzxS0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efa2f6ec"
      },
      "source": [
        "# Find the genre with the highest number of good reviews (predicted_sentiment=1)\n",
        "most_good_genre = genre_sentiment_table[1].idxmax()\n",
        "most_good_count = genre_sentiment_table[1].max()\n",
        "\n",
        "# Find the genre with the highest number of bad reviews (predicted_sentiment=0)\n",
        "most_bad_genre = genre_sentiment_table[0].idxmax()\n",
        "most_bad_count = genre_sentiment_table[0].max()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Genre with the most good reviews: '{most_good_genre}' ({int(most_good_count)} reviews)\")\n",
        "print(f\"Genre with the most bad reviews: '{most_bad_genre}' ({int(most_bad_count)} reviews)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize results\n",
        "\n",
        "Create visualizations to show the distribution of good and bad reviews across different genres. I also implement code to find which movie genre has the most discrepancies, this way we can understand which genre has the most disagreements. Then we create a bar plot to visualize the distribution of good and bad reviews across different genres using the pre-calculated genre_sentiment_table.\n"
      ],
      "metadata": {
        "id": "ITdwm4pk0S7I"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2e1c4b7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Reset index to use 'genre' as a column for seaborn\n",
        "genre_sentiment_plot_data = genre_sentiment_table.reset_index()\n",
        "\n",
        "# Melt the DataFrame to long format for easier plotting with seaborn\n",
        "genre_sentiment_melted = genre_sentiment_plot_data.melt(\n",
        "    id_vars='genre',\n",
        "    value_vars=[0, 1],\n",
        "    var_name='sentiment',\n",
        "    value_name='count'\n",
        ")\n",
        "\n",
        "# Map sentiment labels for better readability in the plot\n",
        "genre_sentiment_melted['sentiment'] = genre_sentiment_melted['sentiment'].map({0: 'Bad', 1: 'Good'})\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.barplot(x='genre', y='count', hue='sentiment', data=genre_sentiment_melted)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Genre')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.title('Distribution of Good and Bad Reviews by Genre')\n",
        "\n",
        "# Rotate x-axis labels for readability\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# Adjust layout and display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate the difference between bad and good reviews\n",
        "genre_sentiment_table['bad_minus_good'] = genre_sentiment_table[0] - genre_sentiment_table[1]\n",
        "\n",
        "# Find the genre with the largest difference (most bad reviews relative to good)\n",
        "most_discrepant_genre = genre_sentiment_table['bad_minus_good'].idxmax()\n",
        "discrepancy_value = genre_sentiment_table['bad_minus_good'].max()\n",
        "\n",
        "# Calculate the ratio of bad to good reviews, handling division by zero\n",
        "genre_sentiment_table['bad_to_good_ratio'] = genre_sentiment_table.apply(\n",
        "    lambda row: row[0] / row[1] if row[1] > 0 else float('inf'), axis=1\n",
        ")\n",
        "\n",
        "# Find the genre with the highest bad to good ratio (worst reviews)\n",
        "worst_genre_by_ratio = genre_sentiment_table['bad_to_good_ratio'].idxmax()\n",
        "worst_ratio_value = genre_sentiment_table['bad_to_good_ratio'].max()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Genre with the largest discrepancy (most bad reviews relative to good): '{most_discrepant_genre}' ({int(discrepancy_value)} difference)\")\n",
        "print(f\"Genre with the worst reviews (highest bad to good ratio): '{worst_genre_by_ratio}' ({worst_ratio_value:.2f} ratio)\")\n",
        "\n",
        "# Rank genres by the ratio of bad to good reviews\n",
        "ranked_genres_by_ratio = genre_sentiment_table.sort_values(by='bad_to_good_ratio', ascending=False)\n",
        "\n",
        "# Display the ranked list including the counts for good and bad reviews\n",
        "print(\"\\nGenres ranked by the ratio of bad to good reviews (showing bad and good counts):\")\n",
        "display(ranked_genres_by_ratio[[0, 1, 'bad_to_good_ratio']].rename(columns={0: 'Bad Reviews', 1: 'Good Reviews'}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The genre with the most good reviews is 'Drama' with 6212 reviews.\n",
        "*   The genre with the most bad reviews is also 'Drama' with 5640 reviews.\n",
        "* The Genre with the most bad reviews compared to good reviews is 'Horror' with a 1173 difference\n",
        "* The Genre with the worst reviews based on ratio is 'Game-Show'"
      ],
      "metadata": {
        "id": "jDiiNwsQ0gh5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Accuracy Check and Cross-validation**"
      ],
      "metadata": {
        "id": "_KKyksXW0pDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming 'y_test' contains the original positive/negative labels (e.g., 1 for positive, 0 for negative)\n",
        "# And 'y_pred' contains the Naive Bayes model's predictions (e.g., 1 for good/positive, 0 for bad/negative)\n",
        "\n",
        "# It is important to ensure that the labels in 'y_test' and 'y_pred' are aligned\n",
        "# and represent the same sentiment (e.g., 1 for positive/good and 0 for negative/bad)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy of the Naive Bayes model: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "fy6RrHpIbTxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates how accurate our Naive Bayes model is at predicting sentiment (good/bad, which corresponds to the positive/negative labels it was trained on) by comparing its predictions (y_pred) to the actual sentiment labels in your test set (y_test)."
      ],
      "metadata": {
        "id": "fFNkNgVe0m-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary libraries"
      ],
      "metadata": {
        "id": "teXQz3yL0vR-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c14d9ee8"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform cross-validation\n",
        "Use `cross_val_score` with your Multinomial Naive Bayes model and the TF-IDF transformed data (`X_train_tfidf` and `y_train`) to get accuracy scores for multiple splits. Perform 5-fold cross-validation on the trained Multinomial Naive Bayes model using the TF-IDF transformed training data and its corresponding labels."
      ],
      "metadata": {
        "id": "7BRH4bVm09fF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "433331d8"
      },
      "source": [
        "cv_scores = cross_val_score(model, X_train_tfidf, y_train, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation accuracy scores:\", cv_scores)\n",
        "print(f\"Mean cross-validation accuracy: {cv_scores.mean():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Analysis Key Findings**\n",
        "\n",
        "Cross-validation using 5 splits was performed on the Multinomial Naive Bayes model with TF-IDF transformed data.\n",
        "The cross-validation accuracy scores across the 5 splits were approximately 0.8918, 0.8935, 0.8889, 0.8919, and 0.8910.\n",
        "The mean cross-validation accuracy is approximately 0.8918.\n",
        "The standard deviation of the cross-validation accuracy is approximately 0.0032, indicating low variability in performance across the different data splits."
      ],
      "metadata": {
        "id": "pjEHmpgG1Fwy"
      }
    }
  ]
}